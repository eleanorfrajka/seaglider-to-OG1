{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a29764-f39c-431c-8e77-fbc6bfe20f01",
   "metadata": {},
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9729d-4db2-419c-bbc2-a2078fc9b7dc",
   "metadata": {},
   "source": [
    "This notebook converts sg015 data from the Labrador Sea in September 2024 into OG1 format.\n",
    "\n",
    "- OG1 format is a newly agreed format (since June 2024) for glider data sets from various platforms (e.g., Seaglider, Slocum, Seaexplorer).  It lives on github here: (https://github.com/OceanGlidersCommunity/OG-format-user-manual).\n",
    "- OG1 manual: https://oceangliderscommunity.github.io/OG-format-user-manual/OG_Format.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8427334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import yaml\n",
    "sys.path.append('/Users/eddifying/Cloudfree/gitlab-cloudfree/seagliderOG1')\n",
    "sys.path.append('/Users/eddifying/Cloudfree/gitlab-cloudfree/glidertest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c437da6-c3b3-4c48-b272-ee5b8ac27f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from seagliderOG1 import fetchers\n",
    "from seagliderOG1 import tools\n",
    "from seagliderOG1 import plotters\n",
    "from seagliderOG1 import readers\n",
    "from seagliderOG1 import convertOG1\n",
    "from seagliderOG1 import vocabularies\n",
    "from seagliderOG1 import  attr_input\n",
    "import glidertest as gt\n",
    "import xarray as xr\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=Warning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd041858-c498-4654-a7c8-1731beb839fc",
   "metadata": {},
   "source": [
    "## Load Seaglider data in native format\n",
    "\n",
    "1. Load multiple basestation files into a list of xarray datasets (`list_datasets`)\n",
    "2. For (each) of these basestation datasets, split the dataset by dimension (`split_ds`)\n",
    "3. Transform into OG1 format: dataset with dims `sg_data_point`\n",
    "    - Change the dimension to `N_MEASUREMENTS`\n",
    "    - Rename variables according to `vocabularies.standard_names` \n",
    "    - Assign attributes to variables (note that this *could* go wrong).  May need a function to handle each one, ideally based on a yaml file for the mission??\n",
    "4. Add missing mandatory variables: \n",
    "    - from `gps_info`, the `LATITUDE_GPS`, `LONGITUDE_GPS` and `TIME_GPS`\n",
    "    - Create `PROFILE_NUMBER` and `PHASE`\n",
    "5. Update attributes\n",
    "6. Retain the variables starting with `sg_cal` and check whether they vary over the mission (shouldn't)\n",
    "6. Add sensors, using information in the `split_ds` with no dimensions\n",
    "    - Need (from sg_cal_constants: `sg_cal` plus `volmax`, `vbd_cnts_per_cc`, `therm_expan`, `t_*`, `mass`, `hd_*`, `ctcor`, `cpcor`, `c_*`, `abs_compress`, `a`, `Tcor`, `Soc`, `Pcor`, `Foffset`)\n",
    "    - Maybe also `reviewed`, `magnetic_variation` (which will change with position), `log_D_FLARE`, `flight_avg_speed_north` and `flight_avg_speed_east` also with `_gsm`, `depth_avg_curr_north` and `depth_avg_curr_east` also with `_gsm`, \n",
    "    `wlbb2f` - means sensor\n",
    "    `sg_cal_mission_title`\n",
    "    `sg_cal_id_str`\n",
    "    `calibcomm_oxygen`\n",
    "    `calibcomm`\n",
    "    `sbe41` means ??\n",
    "    `hdm_qc`\n",
    "    `glider`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac290a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(readers)\n",
    "# Specify the server where data are located\n",
    "if 0:\n",
    "    input_loc = \"https://www.ncei.noaa.gov/data/oceans/glider/seaglider/uw/015/20040924/\"\n",
    "else:\n",
    "    input_loc = '/Users/eddifying/Dropbox/data/sg015-ncei-download/'\n",
    "\n",
    "# Load and concatenate all datasets from the server, optionally specifying the range of profiles to load\n",
    "start_dive = 1\n",
    "end_dive = 10\n",
    "\n",
    "list_datasets = readers.read_basestation(input_loc, 1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(convertOG1)\n",
    "importlib.reload(vocabularies)\n",
    "importlib.reload(plotters)\n",
    "\n",
    "# Pick one basestation xarray dataset to work with\n",
    "ds1 = list_datasets[0]\n",
    "\n",
    "# Convert the dataset and output also variables not included\n",
    "ds_new, attr_warnings, sg_cal, dc_other, dc_log = convertOG1.process_dataset(ds1)\n",
    "\n",
    "# Add these to existing attributes\n",
    "contrib_to_append = {\n",
    "    'contributor_name': 'Eleanor Frajka-Williams',\n",
    "    'contributor_email': 'eleanorfrajka@gmail.com',\n",
    "    'contributor_role': 'Data scientist',\n",
    "    'contributor_role_vocabulary': 'http://vocab.nerc.ac.uk/search_nvs/W08',\n",
    "    'contributing_institutions': 'University of Hamburg - Institute of Oceanography',\n",
    "    'contributing_institutions_vocabulary': 'https://edmo.seadatanet.org/report/1156',\n",
    "    'contributing_institutions_role': 'Data scientist',\n",
    "    'contributing_institutions_role_vocabulary': 'http://vocab.nerc.ac.uk/search_nvs/W08',\n",
    "}\n",
    "\n",
    "# Create the attributes in order\n",
    "ordered_attributes = convertOG1.update_dataset_attributes(ds1, contrib_to_append)\n",
    "\n",
    "for key, value in ordered_attributes.items():\n",
    "    ds_new.attrs[key] = value\n",
    "\n",
    "plotters.show_variables(ds_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b602b08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb9e607",
   "metadata": {},
   "source": [
    "## Save the final dataset\n",
    "\n",
    "At the moment, since the plotters.show_attributes() and plotters.show_variables() are designed to work with netcdf files, I write the xarray dataset to netcdf before loading it and checking the attributes and variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotters.show_attributes(ds_new)\n",
    "## Save the dataset to a NetCDF file\n",
    "# Check if the file exists and delete it if it does\n",
    "output_file = os.path.join('../data', 'sg015_'+ds_new.start_date+'_delayed_temp.nc')\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Save the dataset to a NetCDF file\n",
    "convertOG1.save_dataset(ds_new, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bd819",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plotters)\n",
    "plotters.show_variables(ds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3230c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
